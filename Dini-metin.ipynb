{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9575493-eb4f-413c-afe7-4dd4f332120d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb3e6f-25ab-4a59-a9f3-ff17f1ff2b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def preprocess_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    lemmatized_sentences = []\n",
    "    stemmed_sentences = []\n",
    "    stop_words = set(stopwords.words('turkish'))  # Türkçe stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        # Stopwords'ü çıkarıyoruz\n",
    "        filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "        lemmatized = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "        stemmed = [stemmer.stem(w) for w in filtered_tokens]\n",
    "\n",
    "        lemmatized_sentences.append(lemmatized)\n",
    "        stemmed_sentences.append(stemmed)\n",
    "\n",
    "    return lemmatized_sentences, stemmed_sentences\n",
    "\n",
    "kuran_text = load_text(\"kuran.txt\")\n",
    "incil_text = load_text(\"incil.txt\")\n",
    "\n",
    "kuran_lemma_sentences, kuran_stem_sentences = preprocess_sentences(kuran_text)\n",
    "incil_lemma_sentences, incil_stem_sentences = preprocess_sentences(incil_text)\n",
    "\n",
    "df_kuran = pd.DataFrame({\n",
    "    'Lemmatized': [' '.join(s) for s in kuran_lemma_sentences],\n",
    "    'Stemmed': [' '.join(s) for s in kuran_stem_sentences]\n",
    "})\n",
    "\n",
    "df_incil = pd.DataFrame({\n",
    "    'Lemmatized': [' '.join(s) for s in incil_lemma_sentences],\n",
    "    'Stemmed': [' '.join(s) for s in incil_stem_sentences]\n",
    "})\n",
    "\n",
    "df_kuran.to_csv(\"kuran_stopwords_cumleler.csv\", index=False, encoding='utf-8')\n",
    "df_incil.to_csv(\"incil_stopwords_cumleler.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(\"✅ Stopwords çıkarılmış CSV dosyaları başarıyla kaydedildi: kuran_stopwords_cumleler.csv ve incil_stopwords_cumleler.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71868133-df41-43bb-9a93-4d8f9b688e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a184bc-fcdb-4db7-af9e-942ff612f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "kuran_text = load_text(\"kuran.txt\")\n",
    "incil_text = load_text(\"incil.txt\")\n",
    "\n",
    "print(\"Kur'an Metni Örneği:\", kuran_text[:500])  # İlk 500 karakteri yazdır\n",
    "print(\"İncil Metni Örneği:\", incil_text[:500])  # İlk 500 karakteri yazdır\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584b86f-0b95-48fe-a126-692c7eccd69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kuran_df = pd.read_csv('kuran_cumleler1.csv', encoding='utf-8')\n",
    "incil_df = pd.read_csv('incil_cumleler1.csv', encoding='utf-8')\n",
    "\n",
    "print(\"Kuran Sütunları:\", kuran_df.columns)\n",
    "print(\"İncil Sütunları:\", incil_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2001f-e328-4b44-96da-891a4f04ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kuran_df = pd.read_csv(\"kuran_cumleler1.csv\", encoding=\"utf-8\")\n",
    "incil_df = pd.read_csv(\"incil_cumleler1.csv\", encoding=\"utf-8\")\n",
    "\n",
    "kuran_df = kuran_df.dropna(subset=['Lemmatized'])\n",
    "incil_df = incil_df.dropna(subset=['Lemmatized'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d5b7b-1b5f-4ed3-b8b2-91a7eaab7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "kuran_df = pd.read_csv(\"kuran_cumleler1.csv\", encoding=\"utf-8\")\n",
    "incil_df = pd.read_csv(\"incil_cumleler1.csv\", encoding=\"utf-8\")\n",
    "\n",
    "kuran_df = kuran_df.dropna(subset=['Lemmatized'])\n",
    "incil_df = incil_df.dropna(subset=['Lemmatized'])\n",
    "\n",
    "kuran_sentences = kuran_df['Lemmatized'].tolist()\n",
    "incil_sentences = incil_df['Lemmatized'].tolist()\n",
    "\n",
    "vectorizer_kuran = TfidfVectorizer()\n",
    "vectorizer_incil = TfidfVectorizer()\n",
    "\n",
    "tfidf_kuran = vectorizer_kuran.fit_transform(kuran_sentences)\n",
    "tfidf_incil = vectorizer_incil.fit_transform(incil_sentences)\n",
    "\n",
    "output_directory = \"D:/\"\n",
    "\n",
    "def save_sparse_tfidf_gz(matrix, file_path):\n",
    "    with gzip.open(file_path, 'wt', encoding='utf-8') as f:\n",
    "        for i, row in enumerate(matrix):\n",
    "            coo = row.tocoo()\n",
    "            items = [f\"{col}:{val:.6f}\" for col, val in zip(coo.col, coo.data)]\n",
    "            f.write(' '.join(items) + '\\n')\n",
    "\n",
    "kuran_file = os.path.join(output_directory, 'kuran_tfidf_sparse.csv.gz')\n",
    "incil_file = os.path.join(output_directory, 'incil_tfidf_sparse.csv.gz')\n",
    "\n",
    "save_sparse_tfidf_gz(tfidf_kuran, kuran_file)\n",
    "save_sparse_tfidf_gz(tfidf_incil, incil_file)\n",
    "\n",
    "print(\"Dosyalar başarıyla kaydedildi:\")\n",
    "print(f\"Kuran: {kuran_file}\")\n",
    "print(f\"İncil: {incil_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865965f3-163b-48e6-b993-ba6b6ee44e0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg.special_matrices' (C:\\Users\\veyse\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\linalg\\special_matrices.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gensim\\matutils.py:30\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triu\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'triu' from 'scipy.linalg.basic' (C:\\Users\\veyse\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\linalg\\basic.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Hazırlık: her veri seti ve sütun için adlandırma ve eğitim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gensim\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains interfaces and functionality to compute pair-wise document\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03msimilarities within a corpus of documents.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils, interfaces, corpora, models, similarities\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gensim\\matutils.py:32\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triu\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial_matrices\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triu\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triu_indices\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'triu' from 'scipy.linalg.special_matrices' (C:\\Users\\veyse\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\linalg\\special_matrices.py)"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "datasets = {\n",
    "    'kuran': kuran_df,\n",
    "    'incil': incil_df\n",
    "}\n",
    "columns = ['Lemmatized', 'Stemmed']\n",
    "output_dir = \"D:/\"\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    for column in columns:\n",
    "\n",
    "        sentences = df[column].dropna().apply(lambda x: x.split()).tolist()\n",
    "\n",
    "        for i in range(1, 9):  \n",
    "            model_name = f\"{dataset_name}_{column.lower()}_w2v_model_{i}.model\"\n",
    "            model_path = os.path.join(output_dir, model_name)\n",
    "\n",
    "            print(f\"{model_name} eğitiliyor...\")\n",
    "\n",
    "            model = Word2Vec(\n",
    "                sentences,\n",
    "                vector_size=100, \n",
    "                window=5,\n",
    "                min_count=1,\n",
    "                workers=4,\n",
    "                epochs=10\n",
    "            )\n",
    "\n",
    "            model.save(model_path)\n",
    "            print(f\"{model_name} kaydedildi ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2a4d5-0e2e-4f72-904c-fe51120d3680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
